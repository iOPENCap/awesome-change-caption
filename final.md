file:///c%3A/Users/luoyiyi/learngit/awesome.md {"mtime":1695369401465,"ctime":1695367934712,"size":708,"etag":"3b85snj2imq","orphaned":false,"typeId":""}
# awesome-remote-change-image
The project is currently under construction
### Papers
#### Lite Version
* [**ICIIF 2018**]  [Least Squares Twin Extreme Learning Machine for Pattern Classification](https://link.springer.com/chapter/10.1007/978-981-13-1966-2_50) *Amisha et al.*  [[paper]](https://link.springer.com/book/10.1007/978-981-13-1966-2)<br/>
* [**ScienceDirect 2001**]   [A Survey of Computer Vision-Based Human Motion Capture](https://www.sciencedirect.com/science/article/pii/S107731420090897X) *Thomas et al.*  [[paper]](https://www.sciencedirect.com/)<br/>
* [**ICIIF 2018**]  [A Statistical Approach to Texture Classification from Single Images](https://link.springer.com/article/10.1023/B:VISI.0000046589.39864.ee) *Manik et al.*  [[paper]](https://link.springer.com/)<br/>
*  [**IEEE 2000**]  [Twenty Years of Document Image Analysis in PAMI](https://ieeexplore.ieee.org/document/824820) *George Nagy.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>
*  [**ICIIF 2018**]  [Least Squares Twin Extreme Learning Machine for Pattern Classification](https://link.springer.com/chapter/10.1007/978-981-13-1966-2_50) *Amisha et al.*  [[paper]](https://link.springer.com/book/10.1007/978-981-13-1966-2)<br/>
  *  [**NeurIPS 2022**]  [Learning Distinct and Representative Modes for Image Captioning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html) *Chen et al.*  [[paper]](https://neurips.cc/)<br/> 
  *   [**ACM 2021**]  [Scene Graph with 3D Information for Change Captioning](https://dl.acm.org/doi/10.1145/3474085.3475712) *Liao et al.*  [[paper]](https://dl.acm.org/)<br/>
  *   [**IEEE 2022**]  [ChaLearn Looking at People: IsoGD and ConGD Large-Scale RGB-D Gesture Recognition](https://www.webofscience.com/wos/alldb/full-record/WOS:000798227800076) *Wan et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>  
* [**IEEE 2021**]  [MVTN: Multi-View Transformation Network for 3D Shape Recognition](https://www.webofscience.com/wos/alldb/full-record/WOS:000797698900001) *Hamdi et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>  
* [**IEEE 2021**]  [GLiT: Neural Architecture Search for Global and Local Image Transformer. ](https://www.webofscience.com/wos/alldb/full-record/WOS:000797698900002) *Chen et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>  
* [**IEEE 2022**]  [Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World](https://www.webofscience.com/wos/alldb/full-record/WOS:000898293500002) *Dang et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>  
* [**IEEE 2022**]  [An End-to-End Transformer Model for Crowd Localization](https://www.webofscience.com/wos/alldb/full-record/WOS:000898293500003) *Liang et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>  
* [**IEEE 2022**]  [Large-Scale Pre-training for Person Re-identification with Noisy Labels.](https://www.webofscience.com/wos/alldb/full-record/WOS:000867754202072) *Fu et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>  
* [**IEEE 2022**]  [Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers](https://www.webofscience.com/wos/alldb/full-record/WOS:000867754200002) *Guo et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>
* [**IEEE 2022**]  [CO-SNE: Dimensionality Reduction and Visualization for Hyperbolic Data](https://www.webofscience.com/wos/alldb/full-record/WOS:000867754200003) *Guo et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>
 * [**ACM 2021**]  [Latent Memory-augmented Graph Transformer for Visual Storytelling](https://sc.panda321.com/scholar?hl=zh-cn&q=Latent+Memory-Augmented+Graph+Transformer+for+Visual+Storytelling) *Qi et al.*  [[paper]](https://dl.acm.org/doi/abs/10.1145/3474085.3475236)<br/>
* [**IEEE 2021**]  [Group-based Distinctive Image Captioning with Memory Attention](https://sc.panda321.com/scholar?hl=zh-cn&q=Group-based+Distinctive+Image+Captioning+with+Memory+Attention) *Wang et al.*  [[paper]](https://dl.acm.org/doi/abs/10.1145/3474085.3475236)<br/>
* [**IEEE 2021**]  [Human-like Controllable Image Captioning with Verb-specific Semantic Roles](https://www.webofscience.com/wos/alldb/full-record/WOS:000742075007006) *Chen et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>
* [**IEEE 2021**]  [Towards Accurate Text-based Image Captioning with Content Diversity Exploration](https://www.webofscience.com/wos/alldb/full-record/WOS:000742075002082) *Xu et al.*  [[paper]](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)<br/>
#### Full Version
| Paper info | Description |
|---|---|
| [Least Squares Twin Extreme Learning Machine for Pattern Classification](https://link.springer.com/chapter/10.1007/978-981-13-1966-2_50)<br/>*Rastogi, R. ; Bharti, A.*<br/>> South Asian University, New Delhi, 110021, Delhi, India<br/>> ICIIF 2018<br/>> `Extreme learning machine``Twin support vector machine``Classification``recognition`, <br/>> Cited by 2594 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/1.png" width="300"></div><br/>The paper proposes Least Squares Twin Extreme Learning Machine (LSTELM) for pattern classification . LSTELM formulation solves Extreme Learning Machine (ELM) problem in twin framework.
| [A Survey of Computer Vision-Based Human Motion Capture](https://www.sciencedirect.com/science/article/pii/S107731420090897X)<br/>*Thomas B. Moeslund and Erik Granum*<br/>> Laboratory of Computer Vision and Media Technology, Aalborg University, Niels Jernes Vej 14, Aalborg, 9220, Denmarkf1<br/>> ScienceDirect 2001<br/>> `initialization``tracking``pose estimation``Least squares`, <br/>> Cited by 6 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/2.1.png" width="300"></div><br/>The paper proposes a comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented. The focus is on a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition.<br/> 
| [A Statistical Approach to Texture Classification from Single Images](https://link.springer.com/article/10.1023/B:VISI.0000046589.39864.ee)<br/>*Manik Varma / Andrew Zisserman*<br/>> Robotics Research Group, Department of Engineering Science, University of Oxford, Oxford, OX1 3PJ, UK<br/>> ICIIF 2018<br/>> `Extreme learning machine``Twin support vector machine``Classification``recognition`, <br/>> Cited by 2594 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/3.png" width="300"></div><br/>The paper proposes texture classification from single images obtained under unknown viewpoint and illumination. A statistical approach is developed where textures are modelled by the joint probability distribution of filter responses.
| [Twenty Years of Document Image Analysis in PAMI](https://ieeexplore.ieee.org/document/824820)<br/>*George Nagy*<br/>>  South Asian University, New Delhi, 110021, Delhi, India<br/>> IEEE 2000<br/>> ` Extreme learning machine``Twin support vector machine``Classification``Least squares`, <br/>> Cited by 6 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/4.png" width="300"></div><br/>The contributions to document image analysis of 99 papers published in the E E E Transactions on Pattern Anaiysis and Machine hteiligence (PAMI) are clustered, summarized, interpolated, interpreted, and tactfully evaluated.|
| [A Statistical Approach to Texture Classification from Single Images](https://link.springer.com/article/10.1023/B:VISI.0000046589.39864.ee)<br/>*Manik Varma / Andrew Zisserman*<br/>> Robotics Research Group, Department of Engineering Science, University of Oxford, Oxford, OX1 3PJ, UK<br/>> ICIIF 2018<br/>> `Extreme learning machine``Twin support vector machine``Classification``recognition`, <br/>> Cited by 2594 | <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/paper5.png" width="300"></div><br/>The paper proposes Least Squares Twin Extreme Learning Machine (LSTELM) for pattern classification . LSTELM formulation solves Extreme Learning Machine (ELM) problem in twin framework.
<br/> |
| [Learning Distinct and Representative Modes for Image Captioning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html)<br/>*Qi Chen, Chaorui Deng, Qi Wu*<br/>>  NeurIPS 2022<br/>> `Computer Science``Computer Vision and Pattern Recognition` <br/>> Cited by 1 | <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/paper%206.jpg" width="300"></div><br/>Therefore, this article proposes a method of learning discrete control signals from training corpus. The author believes that each text corresponds to a mode, and the number of modes is a hyperparameter. Each mode in the training phase corresponds to a control signal in the testing phase. The embedded representation of all control signals constitutes the codebook.
| [Scene Graph with 3D Information for Change Captioning](https://dl.acm.org/doi/10.1145/3474085.3475712)<br/>*Zeming Liao, Qingbao Huang, Yu Liang, Mingyi Fu,YiCai,, Qing Li*<br/>>School of Electrical Engineering, Guangxi University, Nanning, Guangxi, China2School of Software Engineering, South China University of Technology, Guangzhou, China3Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China4Key Laboratory of Big Data and Intelligent Robot (SCUT), MOE of China5Guangxi Key Laboratory of Multimedia Communications and Network Technology6Institute of Artificial Intellige<br/>>ACM 2021<br/>> `Computing methodologies``Scene understanding``Natu-ral language generation` <br/>> Cited by 1 | <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/7.png" width="300"></div><br/>>This paper propose a 3D-aware Scene Graph-based Change Captioning (SGCC) model, extracting object semantics and 3D data. This constructs scene graphs for image pairs, with node representations aggregated using graph convolution. SGCC assists observers in quickly identifying changes and is partially immune to viewpoint shifts. Extensive experiments confirm SGCC's competitive performance on CLEVR-Change and Spot-the-Diff datasets, validating our model's effectiveness.
| [ChaLearn Looking at People: IsoGD and ConGD Large-Scale RGB-D Gesture Recognition](https://www.webofscience.com/wos/alldb/full-record/WOS:000798227800076)<br/>*Wan et al*<br/>>  Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China<br/>> IEEE 2022<br/>> ` Gesture recognition``Measurement``Task analysis``Training``Conferences``Computer vision``Bidirectional long short-term memory (Bi-LSTM)``gesture recognition``RGB-D`, <br/>> Cited by 7 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/5.png" width="300"></div><br/>The paper proposes it describes the creation of both benchmark datasets and analyzes the advances in large-scale gesture recognition based on these two datasets.|
| [MVTN: Multi-View Transformation Network for 3D Shape Recognition](https://www.webofscience.com/wos/alldb/full-record/WOS:000797698900001)<br/>*Hamdi et al*<br/>>  King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia<br/>> IEEE 2021<br/>> ` NEURAL-NETWORK`, <br/>> Cited by 23 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/6.png" width="300"></div><br/>The main contribution of this paper is the introduction of a Multi-View Transformation Network (MVTN) that can predict optimal viewpoints to improve 3D shape recognition performance. |
| [GLiT: Neural Architecture Search for Global and Local Image Transformer](https://www.webofscience.com/wos/alldb/full-record/WOS:000797698900002)<br/>*Chen et al*<br/>>  Univ Sydney, Sydney, NSW, Australia<br/>> IEEE 2021<br/>> ` GLiT``Image Recognition``transformer``Neural architecture Search``Local information ``Global Information``Self-attention``multi-head attention mechanism``ImageNet`, <br/>> Cited by 23 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/7.png" width="300"></div><br/>This paper introduces GLiT, the first Neural Architecture Search (NAS) method to find a better transformer architecture specifically for image recognition. |
| [Learning-Based Point Cloud Registration for 6D Object Pose Estimation in the Real World](https://www.webofscience.com/wos/alldb/full-record/WOS:000898293500002)<br/>*Dang et al*<br/>>Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland<br/>> IEEE 2022<br/>> ` 6D object pose estimation``Point cloud registration`, <br/>> Cited by 0 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/8.png" width="300"></div><br/>The main content of the paper is about addressing the challenges faced by learning-based 3D object registration algorithms to estimate the 6D pose of an object from point cloud data in the presence of real-world data. |
| [An End-to-End Transformer Model for Crowd Localization](https://www.webofscience.com/wos/alldb/full-record/WOS:000898293500003)<br/>*Liang et al*<br/>>Huazhong Univ Sci & Technol, Wuhan 430074, Peoples R China<br/>> IEEE 2022<br/>> `Crowd localization``Crowd counting``Transformer`, <br/>> Cited by 3 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/9.png" width="300"></div><br/>The main content of this paper is the proposal of an end-to-end Crowd Localization Transformer (CLTR) model for the task of crowd localization, which aims to predict the location of each instance (head positions) in crowd scenes. |
| [Large-Scale Pre-training for Person Re-identification with Noisy Labels](https://www.webofscience.com/wos/alldb/full-record/WOS:000867754202072)<br/>*Fu et al*<br/>>Microsoft Res, Redmond, WA 98052 USA<br/>> IEEE 2022<br/>> `person re-identification, pre-training, noisy labels, prototype-based contrastive learning, label-guided contrastive learning, deep learning, computer vision`, <br/>> Cited by 2 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/10.png" width="300"></div><br/>This paper proposes a framework for large-scale pre-training for person re-identification (Re-ID) with noisy labels. The framework, called PNL, consists of three learning modules: supervised Re-ID learning, prototype-based contrastive learning, and label-guided contrastive learning. |
| [Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers](https://www.webofscience.com/wos/alldb/full-record/WOS:000867754200002)<br/>* Guo et al*<br/>>UC Berkeley ICSI, Berkeley, CA 94720 USA<br/>> IEEE 2022<br/>> `person re-identification, pre-training, noisy labels, prototype-based contrastive learning, label-guided contrastive learning, deep learning, computer vision`, <br/>> Cited by 41 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/11.png" width="300"></div><br/>This paper introduces a solution to the vanishing gradient problem in training Hyperbolic Neural Networks (HNNs), which is caused by the hybrid architecture connecting Euclidean features to a hyperbolic classifier.|
| [CO-SNE: Dimensionality Reduction and Visualization for Hyperbolic Data](https://www.webofscience.com/wos/alldb/full-record/WOS:000867754200003)<br/>* Guo et al*<br/>>UC Berkeley ICSI, Berkeley, CA 94720 USA<br/>> IEEE 2022<br/>> `CO-SNE, hyper-likelihood distribution, high-dimensional data visualization, non-Euclidean space, hierarchical structure`, <br/>> Cited by 0 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/12.png" width="300"></div><br/>The main content of this article is the proposal of CO-SNE, a method for visualizing high-dimensional hyperbolic data in a low-dimensional hyperbolic space. |
| [Latent Memory-augmented Graph Transformer for Visual Storytelling](https://sc.panda321.com/scholar?hl=zh-cn&q=Latent+Memory-Augmented+Graph+Transformer+for+Visual+Storytelling)<br/>* Qi et al*<br/>>School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China<br/>> IEEE 2021<br/>> `Visual Storytelling, Transformer, Scene Graph, Memory Network`, <br/>> Cited by 0 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/13.png" width="300"></div><br/>The main focus of this paper is on the task of visual storytelling, which involves automatically generating a narrative story for an image stream.|
| [Group-based Distinctive Image Captioning with Memory Attention](https://sc.panda321.com/scholar?hl=zh-cn&q=Group-based+Distinctive+Image+Captioning+with+Memory+Attention)<br/>* Wang et al*<br/>>Department of Computer Science, City University of Hong Kong<br/>> IEEE 2021<br/>> `Image Caption, Distinctiveness, Memory Attention, Similar Image`, <br/>> Cited by 18 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/14.png" width="300"></div><br/>The main content of this paper is the proposal of a Group-based Distinctive Captioning Model (GdisCap) that generates distinctive captions for images by comparing each image with other images in a similar group and highlighting the uniqueness of each image. |
| [Human-like Controllable Image Captioning with Verb-specific Semantic Roles](https://www.webofscience.com/wos/alldb/full-record/WOS:000742075007006)<br/>* Chen et al*<br/>>Zhejiang Univ, Hangzhou, Peoples R China<br/>> IEEE 2021<br/>> `Controllable Image Captioning, Verb-specific Semantic Roles (VSR), Semantic Role Labeling (SRL), Semantic Structure Planner (SSP), Role-shift Caption Generation, Diverse Image Captioning, Visual Grounding`, <br/>> Cited by 17 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/15.png" width="300"></div><br/>The main content of the article is the proposal and implementation of a new control signal for Controllable Image Captioning (CIC) called Verb-specific Semantic Roles (VSR), which considers both event-compatibility and sample-suitability requirements for more human-like controllability. |
| [Towards Accurate Text-based Image Captioning with Content Diversity Exploration](https://www.webofscience.com/wos/alldb/full-record/WOS:000742075002082)<br/>* Xu et al*<br/>>South China Univ Technol, Guangzhou, Peoples R China<br/>> IEEE 2021<br/>> `text-based image captioning, content diversity exploration, anchor proposal module, anchor captioning module, anchor-centred graph, OCR tokens`, <br/>> Cited by 15 | <div align="center"><img src="https://raw.githubusercontent.com/iOPENCap/awesome-change-caption/img-storage/picture/16.png" width="300"></div><br/>The main content of this paper is the proposal of a new method called Anchor-Captioner for text-based image captioning, which aims to generate multiple captions that accurately describe different parts of an image in detail.|

### Datasets
| Dataset info | Description |
|---|---|
| [CLEVR-Change](https://github.com/topics/clevr-change-dataset)<br/>> Stanford University<br/> >English<br/>> 2019<br/>>N/A<br/>> 16000 images | <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/1.png" width="300"></div><br/>The CLEVR-Change dataset is an extension of the CLEVR dataset and includes a series of questions and corresponding images involving changes made to objects in a scene. |
| [TextCaps](https://github.com/MILVLG/textcaps)<br/>> Peking University<br/> >English<br/>> 2021<br/>>N/A<br/>> 28,000 images | <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/2.png" width="300"></div><br/>The TextCaps dataset is a large-scale text-image matching dataset that includes textual descriptions of images from different domains. |
| [COCO](https://cocodataset.org/#home)<br/>> Microsoft<br/> >English<br/>> 2015<br/>> 80 categories<br/>> 328000 images | <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/%E6%95%B0%E6%8D%AE%E9%9B%861.png" width="300"></div><br/>The COCO dataset is widely used for image captioning and object detection tasks. It comprises images from various scenes, often with multiple objects and complex backgrounds, making it an ideal choice for multimodal tasks.  |
| [Conceptual Captions ](https://github.com/google-research-datasets/conceptual-captions)<br/>>  Google Research<br/> >English<br/>> 2018<br/>> N/A <br/>> 12000000 images | <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/dat2.png" width="300"></div><br/>The Conceptual Captions (CC) dataset is a dataset containing (image URL, caption) pairs for the training and evaluation of machine learning image captioning systems. The dataset is available in two versions, about 3.3 million images (CC3M) and 12 million images (CC12M), and automatically collects weakly correlated descriptions from the network through a simple filtering procedure. |
| [SBU Captions](https://huggingface.co/datasets/sbu_captions)<br/>>Stanford University <br/> >English<br/>> 2011| <div align="center"><img src="https://github.com/iOPENCap/awesome-change-caption/blob/master/pic/3(1).png" width="300"></div><br/>The SBU Captions dataset initially used image captions as a retrieval task, containing 1 million image URLs + captions pairs. |

### Popular Implementations
| Code | Paper | Framework |
|---|---|---|
|[Meshed-Memory Transformer for Image Captioning]( https://github.com/aimagelab/meshed-memory-transformer) | [Meshed-Memory Transformer for Image Captioning](https://arxiv.org/abs/1912.08226) | Pytorch |
|[CLIP]( https://github.com/openai/CLIP) | [Contrastive Language-Image Pre-training](https://openai.com/research/clip) | Pytorch |
|[Oscar](https://github.com/aimagelab/meshed-memory-transformer ) | [Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/abs/2004.06165) | Pytorch |
|[UNITER]( https://github.com/ChenRocks/UNITER) | [Learning Universal Image-Text Representations](https://arxiv.org/abs/1909.11740) | Pytorch |
|[UNIMO]( https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO) | [Towards Unified-Modal Understanding and Generation via Text-Image Vision-Language Pre-training](https://readpaper.com/paper/3118641406) | Pytorch |
|[VL-BERT]( https://github.com/jackroos/VL-BERT) | [VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530) | Pytorch |

### SOTA

DataSet:
 
 [CLEVR-Change]([CLEVR-Change](https://github.com/topics/clevr-change-dataset))
| metrics | sota | method | paper |
| --- | --- | --- | --- |
| BLEU-4 | 47.3 | DUDA |[Robust Change Captioning](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Robust_Change_Captioning_ICCV_2019_paper.pdf) |
| METEOR | 33.9 |  DUDA |[Robust Change Captioning](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Robust_Change_Captioning_ICCV_2019_paper.pdf) |
| SPICE | 24.5 |  DUDA |[Robust Change Captioning](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Robust_Change_Captioning_ICCV_2019_paper.pdf) |
| CIDEr | 112.3 | DUDA |[Robust Change Captioning](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Robust_Change_Captioning_ICCV_2019_paper.pdf) |
 [TextCaps](https://github.com/MILVLG/textcaps)
| metrics | sota | method | paper |
| --- | --- | --- | --- |
| BLEU-4 | 23.30 | M4C-Captinoner |[TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/abs/2003.12462) |
| METEOR | 22.00 | M4C-Captinoner |[TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/abs/2003.12462) |
| ROUGE | 46.20 | M4C-Captinoner | [TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/abs/2003.12462) |
| CIDEr | 89.60 | M4C-Captinoner|  [TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/abs/2003.12462) |

 [COCO](https://cocodataset.org/#home)
| metrics | sota | method | paper |
| --- | --- | --- | --- |
| Overall mAP | 49.6 | Dual-Curriculum Teacher |[Dual-Curriculum Teacher for Domain-Inconsistent Object Detection in Autonomous Driving](https://arxiv.org/pdf/2210.08748v1.pdf) |

 [Conceptual Captions](https://github.com/google-research-datasets/conceptual-captions)
| metrics | sota | method | paper |
| --- | --- | --- | --- |
|Params (M) | 156 | MLP + GPT2 tuning  |[TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/pdf/2111.09734v1.pdf) |
| SPICE | 18.5 |  MLP + GPT2 tuning  |[TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/pdf/2111.09734v1.pdf) |
| ROUGE-L | 26.7 |  MLP + GPT2 tuning  |[TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/pdf/2111.09734v1.pdf) |
| CIDEr | 87.26 |  MLP + GPT2 tuning  |[TextCaps: a Dataset for Image Captioning with Reading Comprehension](https://arxiv.org/pdf/2111.09734v1.pdf) |

 [SBU Captions](https://huggingface.co/datasets/sbu_captions)
| metrics | sota | method | paper |
| --- | --- | --- | --- |
| BLEU | 0.1259 | Global + Content Matching (linear SVM) | [Im2Text: Describing Images Using 1 Million Captioned Photographs](https://proceedings.neurips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf) |


